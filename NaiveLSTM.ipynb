{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc453bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauren/mambaforge/envs/pytorch_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Re-loads all imports every time the cell is ran. \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# Sklearn tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Neural Networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers.csv_logs import CSVLogger\n",
    "\n",
    "# Plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8146577",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesDataset(Dataset):   \n",
    "    '''\n",
    "    Custom Dataset subclass. \n",
    "    Serves as input to DataLoader to transform X \n",
    "      into sequence data using rolling window. \n",
    "    DataLoader using this dataset will output batches \n",
    "      of `(batch_size, seq_len, n_features)` shape.\n",
    "    Suitable as an input to RNNs. \n",
    "    '''\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, seq_len: int = 1):\n",
    "        self.X = torch.tensor(X).float()\n",
    "        self.y = torch.tensor(y).float()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.__len__() - (self.seq_len-1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.X[index:index+self.seq_len], self.y[index+self.seq_len-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b5d5695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleAcceleratorDataModule(pl.LightningDataModule):\n",
    "    '''\n",
    "    PyTorch Lighting DataModule subclass:\n",
    "    https://pytorch-lightning.readthedocs.io/en/latest/datamodules.html\n",
    "\n",
    "    Serves the purpose of aggregating all data loading \n",
    "      and processing work in one place.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, seq_len = 1, batch_size = 128, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_val = None\n",
    "        self.y_val = None\n",
    "        self.X_test = None\n",
    "        self.X_test = None\n",
    "        self.columns = None\n",
    "        self.preprocessing = None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):        \n",
    "        \n",
    "        df = pd.read_pickle('pickled_df_ALL.pk').diff()\n",
    "\n",
    "\n",
    "        X = df[['Fwd1Amp',  'Fwd2Amp', 'CavAmp', 'SpareAmp', 'LP_Amp','RevAmp','Fwd1Phs', 'Fwd2Phs', 'Cavphs', 'SparePhs',  'LP_Phase', 'Rev_Phs']]\n",
    "        y = df['SCam3_Gauss_yCentroid']\n",
    "        \n",
    "        X = X.dropna()\n",
    "        y = y.dropna()\n",
    "        self.columns = X.columns\n",
    "\n",
    "\n",
    "        X_cv, X_test, y_cv, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, shuffle=False\n",
    "        )\n",
    "    \n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_cv, y_cv, test_size=0.25, shuffle=False\n",
    "        )\n",
    "\n",
    "        preprocessing = RobustScaler()\n",
    "        preprocessing.fit(X_train)\n",
    "\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.X_train = preprocessing.transform(X_train)\n",
    "            self.y_train = y_train.values.reshape((-1, 1))\n",
    "            self.X_val = preprocessing.transform(X_val)\n",
    "            self.y_val = y_val.values.reshape((-1, 1))\n",
    "\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.X_test = preprocessing.transform(X_test)\n",
    "            self.y_test = y_test.values.reshape((-1, 1))\n",
    "        \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = TimeseriesDataset(self.X_train, \n",
    "                                          self.y_train, \n",
    "                                          seq_len=self.seq_len)\n",
    "        train_loader = DataLoader(train_dataset, \n",
    "                                  batch_size = self.batch_size, \n",
    "                                  shuffle = False, \n",
    "                                  num_workers = self.num_workers)\n",
    "        \n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = TimeseriesDataset(self.X_val, \n",
    "                                        self.y_val, \n",
    "                                        seq_len=self.seq_len)\n",
    "        val_loader = DataLoader(val_dataset, \n",
    "                                batch_size = self.batch_size, \n",
    "                                shuffle = False, \n",
    "                                num_workers = self.num_workers)\n",
    "\n",
    "        return val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dataset = TimeseriesDataset(self.X_test, \n",
    "                                         self.y_test, \n",
    "                                         seq_len=self.seq_len)\n",
    "        test_loader = DataLoader(test_dataset, \n",
    "                                 batch_size = self.batch_size, \n",
    "                                 shuffle = False, \n",
    "                                 num_workers = self.num_workers)\n",
    "\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db54d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRegressor(pl.LightningModule):\n",
    "    '''\n",
    "    Standard PyTorch Lightning module:\n",
    "    https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 n_features, \n",
    "                 hidden_size, \n",
    "                 seq_len, \n",
    "                 batch_size,\n",
    "                 num_layers, \n",
    "                 dropout, \n",
    "                 learning_rate,\n",
    "                 criterion):\n",
    "        super(LSTMRegressor, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.criterion = criterion\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=n_features, \n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, \n",
    "                            dropout=dropout, \n",
    "                            batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # lstm_out = (batch_size, seq_len, hidden_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        y_pred = self.linear(lstm_out[:,-1])\n",
    "        return y_pred\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        result = self.criterion(y_hat, y)\n",
    "        return result\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        result= self.criterion(y_hat, y)\n",
    "        return result\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        result = self.criterion(y_hat, y)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc7a3e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "All parameters are aggregated in one place.\n",
    "This is useful for reporting experiment params to experiment tracking software\n",
    "'''\n",
    "\n",
    "p = dict(\n",
    "    seq_len = 24,\n",
    "    batch_size = 70, \n",
    "    criterion = nn.MSELoss(),\n",
    "    max_epochs = 100,\n",
    "    n_features = 12,\n",
    "    hidden_size =24,\n",
    "    num_layers = 1,\n",
    "    dropout = 0.2,\n",
    "    learning_rate = 0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02c8f5b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n",
      "/Users/lauren/mambaforge/envs/pytorch_env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=2)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/Users/lauren/mambaforge/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/Users/lauren/mambaforge/envs/pytorch_env/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "/Users/lauren/mambaforge/envs/pytorch_env/lib/python3.8/site-packages/pytorch_lightning/loggers/csv_logs.py:57: UserWarning: Experiment logs directory ./lstm/0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | criterion | MSELoss | 0     \n",
      "1 | lstm      | LSTM    | 3.6 K \n",
      "2 | linear    | Linear  | 25    \n",
      "--------------------------------------\n",
      "3.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.7 K     Total params\n",
      "0.015     Total estimated model params size (MB)\n",
      "/Users/lauren/mambaforge/envs/pytorch_env/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /Users/lauren/marinetti/lstm/0/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauren/mambaforge/envs/pytorch_env/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Global seed set to 1\n",
      "/Users/lauren/mambaforge/envs/pytorch_env/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  75%|█████████   | 96/128 [00:00<00:00, 105.07it/s, loss=43.9, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|███████████| 128/128 [00:01<00:00, 123.48it/s, loss=43.9, v_num=0]\u001b[A\n",
      "Epoch 1:  75%|█████████   | 96/128 [00:00<00:00, 104.54it/s, loss=43.9, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1:  94%|██████████▎| 120/128 [00:01<00:00, 118.95it/s, loss=43.9, v_num=0]\u001b[A\n",
      "Epoch 1: 100%|███████████| 128/128 [00:01<00:00, 121.48it/s, loss=43.9, v_num=0]\u001b[A\n",
      "Epoch 2:  75%|█████████   | 96/128 [00:00<00:00, 107.45it/s, loss=43.8, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2:  94%|██████████▎| 120/128 [00:00<00:00, 121.25it/s, loss=43.8, v_num=0]\u001b[A\n",
      "Epoch 2: 100%|███████████| 128/128 [00:01<00:00, 123.09it/s, loss=43.8, v_num=0]\u001b[A\n",
      "Epoch 3:  75%|█████████   | 96/128 [00:00<00:00, 108.65it/s, loss=43.8, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3:  94%|██████████▎| 120/128 [00:00<00:00, 124.07it/s, loss=43.8, v_num=0]\u001b[A\n",
      "Epoch 3: 100%|███████████| 128/128 [00:01<00:00, 126.79it/s, loss=43.8, v_num=0]\u001b[A\n",
      "Epoch 4:  75%|█████████   | 96/128 [00:00<00:00, 105.90it/s, loss=43.8, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|███████████| 128/128 [00:01<00:00, 120.28it/s, loss=43.8, v_num=0]\u001b[A\n",
      "Epoch 5:  75%|█████████   | 96/128 [00:00<00:00, 107.05it/s, loss=43.7, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5:  94%|██████████▎| 120/128 [00:00<00:00, 123.32it/s, loss=43.7, v_num=0]\u001b[A\n",
      "Epoch 5: 100%|███████████| 128/128 [00:01<00:00, 125.62it/s, loss=43.7, v_num=0]\u001b[A\n",
      "Epoch 6:  75%|█████████▊   | 96/128 [00:00<00:00, 98.26it/s, loss=43.6, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|███████████| 128/128 [00:01<00:00, 111.37it/s, loss=43.6, v_num=0]\u001b[A\n",
      "Epoch 7:  75%|█████████   | 96/128 [00:00<00:00, 112.10it/s, loss=43.3, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7:  94%|██████████▎| 120/128 [00:00<00:00, 126.97it/s, loss=43.3, v_num=0]\u001b[A\n",
      "Epoch 7: 100%|███████████| 128/128 [00:00<00:00, 129.44it/s, loss=43.3, v_num=0]\u001b[A\n",
      "Epoch 8:  75%|█████████   | 96/128 [00:00<00:00, 117.60it/s, loss=42.5, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8:  94%|██████████▎| 120/128 [00:00<00:00, 133.12it/s, loss=42.5, v_num=0]\u001b[A\n",
      "Epoch 8: 100%|███████████| 128/128 [00:00<00:00, 135.56it/s, loss=42.5, v_num=0]\u001b[A\n",
      "Epoch 9:  75%|██████████▌   | 96/128 [00:00<00:00, 113.36it/s, loss=41, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9:  94%|████████████▏| 120/128 [00:00<00:00, 129.78it/s, loss=41, v_num=0]\u001b[A\n",
      "Epoch 9: 100%|█████████████| 128/128 [00:00<00:00, 133.08it/s, loss=41, v_num=0]\u001b[A\n",
      "Epoch 10:  75%|████████▎  | 96/128 [00:00<00:00, 119.98it/s, loss=40.3, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                        | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 128/128 [00:00<00:00, 135.51it/s, loss=40.3, v_num=0]\u001b[A\n",
      "Epoch 11:  75%|████████▎  | 96/128 [00:00<00:00, 114.75it/s, loss=40.2, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11:  94%|█████████▍| 120/128 [00:00<00:00, 130.72it/s, loss=40.2, v_num=0]\u001b[A\n",
      "Epoch 11: 100%|██████████| 128/128 [00:00<00:00, 133.42it/s, loss=40.2, v_num=0]\u001b[A\n",
      "Epoch 12:  75%|████████▎  | 96/128 [00:00<00:00, 116.16it/s, loss=39.8, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12:  94%|█████████▍| 120/128 [00:00<00:00, 131.80it/s, loss=39.8, v_num=0]\u001b[A\n",
      "Epoch 12: 100%|██████████| 128/128 [00:00<00:00, 134.21it/s, loss=39.8, v_num=0]\u001b[A\n",
      "Epoch 13:  75%|████████▎  | 96/128 [00:00<00:00, 117.82it/s, loss=39.6, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13:  94%|█████████▍| 120/128 [00:00<00:00, 134.27it/s, loss=39.6, v_num=0]\u001b[A\n",
      "Epoch 13: 100%|██████████| 128/128 [00:00<00:00, 137.58it/s, loss=39.6, v_num=0]\u001b[A\n",
      "Epoch 14:  75%|████████▎  | 96/128 [00:00<00:00, 121.43it/s, loss=39.5, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14:  94%|█████████▍| 120/128 [00:00<00:00, 137.84it/s, loss=39.5, v_num=0]\u001b[A\n",
      "Epoch 14: 100%|██████████| 128/128 [00:00<00:00, 139.04it/s, loss=39.5, v_num=0]\u001b[A\n",
      "Epoch 15:  75%|████████▎  | 96/128 [00:00<00:00, 113.88it/s, loss=39.1, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15:  94%|█████████▍| 120/128 [00:00<00:00, 127.39it/s, loss=39.1, v_num=0]\u001b[A\n",
      "Epoch 15: 100%|██████████| 128/128 [00:00<00:00, 130.31it/s, loss=39.1, v_num=0]\u001b[A\n",
      "Epoch 16:  75%|████████▎  | 96/128 [00:00<00:00, 111.09it/s, loss=38.7, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16:  94%|█████████▍| 120/128 [00:00<00:00, 125.93it/s, loss=38.7, v_num=0]\u001b[A\n",
      "Epoch 16: 100%|██████████| 128/128 [00:00<00:00, 128.71it/s, loss=38.7, v_num=0]\u001b[A\n",
      "Epoch 17:  75%|████████▎  | 96/128 [00:00<00:00, 108.07it/s, loss=38.4, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17:  94%|█████████▍| 120/128 [00:00<00:00, 122.72it/s, loss=38.4, v_num=0]\u001b[A\n",
      "Epoch 17: 100%|██████████| 128/128 [00:01<00:00, 124.96it/s, loss=38.4, v_num=0]\u001b[A\n",
      "Epoch 18:  75%|█████████▊   | 96/128 [00:00<00:00, 117.88it/s, loss=38, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18:  94%|███████████▎| 120/128 [00:00<00:00, 132.20it/s, loss=38, v_num=0]\u001b[A\n",
      "Epoch 18: 100%|████████████| 128/128 [00:00<00:00, 135.75it/s, loss=38, v_num=0]\u001b[A\n",
      "Epoch 19:  75%|████████▎  | 96/128 [00:00<00:00, 108.00it/s, loss=37.9, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19:  94%|█████████▍| 120/128 [00:00<00:00, 122.91it/s, loss=37.9, v_num=0]\u001b[A\n",
      "Epoch 19: 100%|██████████| 128/128 [00:01<00:00, 125.84it/s, loss=37.9, v_num=0]\u001b[A\n",
      "Epoch 20:  75%|████████▎  | 96/128 [00:00<00:00, 120.18it/s, loss=37.6, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20:  94%|█████████▍| 120/128 [00:00<00:00, 135.21it/s, loss=37.6, v_num=0]\u001b[A\n",
      "Epoch 20: 100%|██████████| 128/128 [00:00<00:00, 137.14it/s, loss=37.6, v_num=0]\u001b[A\n",
      "Epoch 21:  75%|████████▎  | 96/128 [00:00<00:00, 108.37it/s, loss=39.3, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21:  94%|█████████▍| 120/128 [00:00<00:00, 124.20it/s, loss=39.3, v_num=0]\u001b[A\n",
      "Epoch 21: 100%|██████████| 128/128 [00:01<00:00, 126.92it/s, loss=39.3, v_num=0]\u001b[A\n",
      "Epoch 22:  75%|████████▎  | 96/128 [00:00<00:00, 117.02it/s, loss=37.5, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22:  94%|█████████▍| 120/128 [00:00<00:00, 132.51it/s, loss=37.5, v_num=0]\u001b[A\n",
      "Epoch 22: 100%|██████████| 128/128 [00:00<00:00, 135.27it/s, loss=37.5, v_num=0]\u001b[A\n",
      "Epoch 23:  75%|████████▎  | 96/128 [00:00<00:00, 118.78it/s, loss=37.2, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23:  94%|█████████▍| 120/128 [00:00<00:00, 133.11it/s, loss=37.2, v_num=0]\u001b[A\n",
      "Epoch 23: 100%|██████████| 128/128 [00:00<00:00, 135.83it/s, loss=37.2, v_num=0]\u001b[A\n",
      "Epoch 24:  75%|████████▎  | 96/128 [00:00<00:00, 117.79it/s, loss=36.7, v_num=0]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24:  94%|█████████▍| 120/128 [00:00<00:00, 131.83it/s, loss=36.7, v_num=0]\u001b[A\n",
      "Epoch 24: 100%|██████████| 128/128 [00:00<00:00, 134.87it/s, loss=36.7, v_num=0]\u001b[A\n",
      "Epoch 25:   9%|█          | 12/128 [00:00<00:01, 113.71it/s, loss=40.1, v_num=0]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lauren/mambaforge/envs/pytorch_env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "/Users/lauren/mambaforge/envs/pytorch_env/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  50%|████████████████▌                | 16/32 [00:00<00:00, 146.38it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|█████████████████████████████████| 32/32 [00:00<00:00, 187.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(1)\n",
    "\n",
    "csv_logger = CSVLogger('./', name='lstm', version='0'),\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=p['max_epochs'],\n",
    "    logger=csv_logger,\n",
    "    progress_bar_refresh_rate=2,\n",
    ")\n",
    "\n",
    "model = LSTMRegressor(\n",
    "    n_features = p['n_features'],\n",
    "    hidden_size = p['hidden_size'],\n",
    "    seq_len = p['seq_len'],\n",
    "    batch_size = p['batch_size'],\n",
    "    criterion = p['criterion'],\n",
    "    num_layers = p['num_layers'],\n",
    "    dropout = p['dropout'],\n",
    "    learning_rate = p['learning_rate']\n",
    ")\n",
    "\n",
    "dm = ParticleAcceleratorDataModule(\n",
    "    seq_len = p['seq_len'],\n",
    "    batch_size = p['batch_size']\n",
    ")\n",
    "\n",
    "dm.setup()\n",
    "\n",
    "trainer.fit(model, dm)\n",
    "trainer.test(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d6d7d0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './lstm/0/metrics.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./lstm/0/metrics.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m metrics[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m]][\u001b[38;5;241m~\u001b[39mnp\u001b[38;5;241m.\u001b[39misnan(metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n\u001b[1;32m      3\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m metrics[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m]][\u001b[38;5;241m~\u001b[39mnp\u001b[38;5;241m.\u001b[39misnan(metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n",
      "File \u001b[0;32m~/mambaforge/envs/pytorch_env/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/pytorch_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/pytorch_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/mambaforge/envs/pytorch_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/pytorch_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/mambaforge/envs/pytorch_env/lib/python3.8/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './lstm/0/metrics.csv'"
     ]
    }
   ],
   "source": [
    "metrics = pd.read_csv('./lstm/0/metrics.csv')\n",
    "train_loss = metrics[['train_loss', 'step', 'epoch']][~np.isnan(metrics['train_loss'])]\n",
    "val_loss = metrics[['val_loss', 'epoch']][~np.isnan(metrics['val_loss'])]\n",
    "test_loss = metrics['test_loss'].iloc[-1]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5), dpi=100)\n",
    "axes[0].set_title('Train loss per batch')\n",
    "axes[0].plot(train_loss['step'], train_loss['train_loss'])\n",
    "axes[1].set_title('Validation loss per epoch')\n",
    "axes[1].plot(val_loss['epoch'], val_loss['val_loss'], color='orange')\n",
    "plt.show(block = True)\n",
    "\n",
    "print('MSE:')\n",
    "print(f\"Train loss: {train_loss['train_loss'].iloc[-1]:.3f}\")\n",
    "print(f\"Val loss:   {val_loss['val_loss'].iloc[-1]:.3f}\")\n",
    "print(f'Test loss:  {test_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da040eec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
